{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Kaggle data management api"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# !pip install kaggle # now download the api token and store it to /home/ec2-user/.kaggle/kaggle.json\r\n",
    "# # # !cd /home/ec2-user/SageMaker\r\n",
    "# !mkdir /home/ec2-user/.kaggle/\r\n",
    "# !mv /home/ec2-user/SageMaker/kaggle.json /home/ec2-user/.kaggle/kaggle.json\r\n",
    "# !chmod 600 /home/ec2-user/.kaggle/kaggle.json # for privacy\r\n",
    "# !zip -r -0 -q data.zip /home/ec2-user/SageMaker/data/\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Append the project directory in sys.path for smoother execution"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "from sys import path\r\n",
    "\r\n",
    "source_proj_dir='/home/ec2-user/SageMaker/Retinopathy2'\r\n",
    "# path.append(source_proj_dir)\r\n",
    "print(path)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['', '/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python36.zip', '/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6', '/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/lib-dynload', '/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages', '/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/IPython/extensions', '/home/ec2-user/.ipython']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "import os\r\n",
    "os.chdir(\"/home/ec2-user/SageMaker/Retinopathy2\")\r\n",
    "# !kaggle competitions download -c aptos2019-blindness-detection\r\n",
    "# !unzip -q /home/ec2-user/SageMaker/aptos2019-blindness-detection.zip -d /home/ec2-user/SageMaker/data1  #-q for quitely no verbose\r\n",
    "# mv /home/ec2-user/aptos2019-blindness-detection.zip /home/ec2-user/SageMaker/"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# !git clone https://github.com/RamsteinWR/Retinopathy2.git\r\n",
    "# !rm -rf Retinopathy2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessing the authenication management"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "%%time\r\n",
    "import boto3\r\n",
    "import botocore\r\n",
    "from botocore.exceptions import ClientError\r\n",
    "from tqdm import tqdm\r\n",
    "import os \r\n",
    "import urllib.request\r\n",
    "import re\r\n",
    "import sagemaker\r\n",
    "from sagemaker import get_execution_role\r\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\r\n",
    "\r\n",
    "role = get_execution_role()\r\n",
    "\r\n",
    "bucket = \"dataset-retinopathy\"\r\n",
    "region_name=\"us-east-1\"\r\n",
    "aws_access_key_id = \"agduyf\"\r\n",
    "aws_secret_access_key = \"5hagjbfajy\"\r\n",
    "\r\n",
    "training_image = get_image_uri(boto3.Session().region_name, 'image-classification')\r\n",
    "\r\n",
    "def download(url):\r\n",
    "    filename = url.split(\"/\")[-1]\r\n",
    "    if not os.path.exists(filename):\r\n",
    "        urllib.request.urlretrieve(url, filename)\r\n",
    "\r\n",
    "def upload_to_s3(channel, file):\r\n",
    "    s3 = boto3.resource('s3',\r\n",
    "                             region_name=region_name,\r\n",
    "                             aws_access_key_id=aws_access_key_id,\r\n",
    "                             aws_secret_access_key=aws_secret_access_key)    \r\n",
    "    data = open(file, \"rb\")\r\n",
    "    key = channel + '/' + file\r\n",
    "    print(\"Uploading file {} to s3://{}/{}\".format(file, bucket, channel))\r\n",
    "    s3.Bucket(bucket).put_object(Key=key, Body=data)\r\n",
    "\r\n",
    "def download(url):\r\n",
    "    filename = url.split(\"/\")[-1]\r\n",
    "    if not os.path.exists(filename):\r\n",
    "        urllib.request.urlretrieve(url, filename)\r\n",
    "\r\n",
    "def upload_to_s3(channel, file):\r\n",
    "    s3 = boto3.resource('s3')\r\n",
    "    data = open(file, \"rb\")\r\n",
    "    key = channel + '/' + file\r\n",
    "    s3.Bucket(bucket).put_object(Key=key, Body=data)\r\n",
    "\r\n",
    "def upload_dir_to_s3(bucket, s3_folder, dir_to_upload):\r\n",
    "    s3_client = boto3.client('s3',\r\n",
    "                             region_name=region_name,\r\n",
    "                             aws_access_key_id=aws_access_key_id,\r\n",
    "                             aws_secret_access_key=aws_secret_access_key)\r\n",
    "    print(\"Uploading {} to s3://{}/{}\".format(dir_to_upload, bucket, s3_folder))\r\n",
    "    # enumerate local files recursively\r\n",
    "    for root, dirs, files in os.walk(dir_to_upload):\r\n",
    "        for filename in tqdm(files):\r\n",
    "            # construct the full local path\r\n",
    "            local_path = os.path.join(root, filename)\r\n",
    "            # construct the full Dropbox path\r\n",
    "            relative_path = os.path.relpath(local_path, dir_to_upload)\r\n",
    "            s3_path = os.path.join(s3_folder, relative_path).replace(\"\\\\\", \"/\")\r\n",
    "            try:\r\n",
    "                s3_client.head_object(Bucket=bucket, Key=s3_path)\r\n",
    "                print(\"Path found on S3! Deleting %s...\" % s3_path)\r\n",
    "                try:\r\n",
    "                    s3_client.delete_object(Bucket=bucket, Key=s3_path)\r\n",
    "                    try:\r\n",
    "#                         print(\"Uploading {} to s3://{}/{}\".format(dir_to_upload, bucket, s3_path)\r\n",
    "                        s3_client.upload_file(local_path, Bucket=bucket, Key=s3_path)\r\n",
    "                    except ClientError as e:\r\n",
    "                        logging.error(e)\r\n",
    "                except:\r\n",
    "                    print(\"Unable to delete from s3 %s...\" % s3_path)\r\n",
    "            except:\r\n",
    "                try:\r\n",
    "                    s3_client.upload_file(local_path, Bucket=bucket, Key=s3_path)\r\n",
    "                except ClientError as e:\r\n",
    "                    logging.error(e)\r\n",
    "    print(\"Upload completed successfully.\")\r\n",
    "    \r\n",
    "def download_dir(s3_folder, local_path, bucket=\"\"):\r\n",
    "    \"\"\"\r\n",
    "    params:\r\n",
    "    - s3_folder: pattern to match in s3\r\n",
    "    - local_path: local_path path to folder in which to place files\r\n",
    "    - bucket: s3 bucket with target contents\r\n",
    "    - client: initialized s3 client object\r\n",
    "    \"\"\"\r\n",
    "    client = boto3.client('s3', region_name=region_name)\r\n",
    "    keys = []\r\n",
    "    dirs = []\r\n",
    "    next_token = ''\r\n",
    "    base_kwargs = {\r\n",
    "        'Bucket': bucket,\r\n",
    "        'Prefix': s3_folder,\r\n",
    "    }\r\n",
    "    while next_token is not None:\r\n",
    "        kwargs = base_kwargs.copy()\r\n",
    "        if next_token != '':\r\n",
    "            kwargs.update({'ContinuationToken': next_token})\r\n",
    "        results = client.list_objects_v2(**kwargs)\r\n",
    "        contents = results.get('Contents')\r\n",
    "        for i in contents:\r\n",
    "            k = i.get('Key')\r\n",
    "            if k[-1] != '/':\r\n",
    "                keys.append(k)\r\n",
    "            else:\r\n",
    "                dirs.append(k)\r\n",
    "        next_token = results.get('NextContinuationToken')\r\n",
    "    for d in dirs:\r\n",
    "        dest_pathname = os.path.join(local_path, d)\r\n",
    "        if not os.path.exists(os.path.dirname(dest_pathname)):\r\n",
    "            os.makedirs(os.path.dirname(dest_pathname))\r\n",
    "    print(\"{} files found in {} directories. Downloading now...\".format(len(keys), len(dirs)))\r\n",
    "    for k in tqdm(keys):\r\n",
    "        dest_pathname = os.path.join(local_path, k)\r\n",
    "        if not os.path.exists(os.path.dirname(dest_pathname)):\r\n",
    "            os.makedirs(os.path.dirname(dest_pathname))\r\n",
    "        try:\r\n",
    "#             print(\"Downloading {}\".format(dest_pathname))\r\n",
    "            client.download_file(bucket, k, dest_pathname)\r\n",
    "        except botocore.exceptions.ClientError as e:\r\n",
    "            if e.response['Error']['Code'] == \"404\":\r\n",
    "                print(\"The object does not exist.\")\r\n",
    "            else:\r\n",
    "                raise\r\n",
    "    print(\"{} files downloaded successfully.\".format(len(keys)))\r\n",
    "def download_from_s3(s3_filename, local_path=\"0.fbx\"):\r\n",
    "    s3_client = boto3.client('s3',\r\n",
    "                             region_name=region_name,\r\n",
    "                             aws_access_key_id=aws_access_key_id,\r\n",
    "                             aws_secret_access_key=aws_secret_access_key)\r\n",
    "    print(\"Downloading file {} to {}\".format(s3_filename, local_path))\r\n",
    "    try:\r\n",
    "        s3_client.download_file(bucket, Key=s3_filename, Filename=local_path)\r\n",
    "        # it waits till the download completes then moves the execution to forward\r\n",
    "    except botocore.exceptions.ClientError as e:\r\n",
    "        if e.response['Error']['Code'] == \"404\":\r\n",
    "            print(\"The object does not exist.\")\r\n",
    "        else:\r\n",
    "            raise\r\n",
    "# download('http://data.lip6.fr/cadene/pretrainedmodels/se_resnext50_32x4d-a260b3a4.pth')\r\n",
    "# upload_to_s3(\"pretrained\", 'se_resnext50_32x4d-a260b3a4.pth')\r\n",
    "# download_dir(s3_folder='aptos-2015/train_images_768/', local_path='/home/ec2-user/SageMaker/data/aptos-2015/train_images_768/', bucket=bucket)\r\n",
    "# download_dir(s3_folder='aptos-2015/test_images_768/', local_path='/home/ec2-user/SageMaker/data/aptos-2015/test_images_768/', bucket=bucket)\r\n",
    "# download_from_s3(s3_filename='pytorch-training-2020-12-29-09-38-13-247/source/sourcedir.tar.gz', local_path=\"/home/ec2-user/SageMaker/checkpoint/sourcedir.tar.gz\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "The method get_image_uri has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "Defaulting to the only supported framework/algorithm version: 1. Ignoring framework/algorithm version: 1.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 44.7 ms, sys: 0 ns, total: 44.7 ms\n",
      "Wall time: 75 ms\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# !pip install -r requirement.txt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "import os\r\n",
    "os.chdir('/home/ec2-user/SageMaker/checkpoint/')\r\n",
    "!tar -xf /home/ec2-user/SageMaker/checkpoint/sourcedir.tar.gz"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id='idg4c.1'></a>\n",
    "## Dependencies\n",
    "___\n",
    "### import packages and check SageMaker version"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "# !pip install -U sagemaker\r\n",
    "\r\n",
    "import json\r\n",
    "import torch\r\n",
    "import tarfile\r\n",
    "import pickle\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import torchvision as tv\r\n",
    "import pathlib                          # Path management tool (standard library)\r\n",
    "import subprocess                       # Runs shell commands via Python (standard library)\r\n",
    "import sagemaker                        # SageMaker Python SDK\r\n",
    "from sagemaker.pytorch import PyTorch   # PyTorch Estimator for TensorFlow"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id='idg4c.4'></a>\n",
    "## Estimator configuration\n",
    "___\n",
    "\n",
    "These define the the resources to use for training and how they are configured. Here are some important one to single out:\n",
    "\n",
    "* **entry_point (str)** – Path (absolute or relative) to the Python source file which should be executed as the entry point to training. If source_dir is specified, then entry_point must point to a file located at the root of source_dir.\n",
    "\n",
    "* **framework_version (str)** – PyTorch version you want to use for executing your model training code. Defaults to None. Required unless image_uri is provided. List of supported versions: https://github.com/aws/sagemaker-python-sdk#pytorch-sagemaker-estimators.\n",
    "\n",
    "* **py_version (str)** – Python version you want to use for executing your model training code. One of ‘py2’ or ‘py3’. Defaults to None. Required unless image_uri is provided.\n",
    "\n",
    "* **source_dir (str)** – Path (absolute, relative or an S3 URI) to a directory with any other training source code dependencies aside from the entry point file (default: None). If source_dir is an S3 URI, it must point to a tar.gz file. Structure within this directory are preserved when training on Amazon SageMaker.\n",
    "\n",
    "* **dependencies (list[str])** – A list of paths to directories (absolute or relative) with any additional libraries that will be exported to the container (default: []). The library folders will be copied to SageMaker in the same folder where the entrypoint is copied. If ‘git_config’ is provided, ‘dependencies’ should be a list of relative locations to directories with any additional libraries needed in the Git repo.\n",
    "\n",
    "* **git_config (dict[str, str])** – Git configurations used for cloning files, including repo, branch, commit, 2FA_enabled, username, password and token. The repo field is required. All other fields are optional. repo specifies the Git repository where your training script is stored. If you don’t provide branch, the default value ‘master’ is used. If you don’t provide commit, the latest commit in the specified branch is used.\n",
    "\n",
    "* **role (str)** – An AWS IAM role (either name or full ARN). The Amazon SageMaker training jobs and APIs that create Amazon SageMaker endpoints use this role to access training data and model artifacts. After the endpoint is created, the inference code might use the IAM role, if it needs to access an AWS resource.\n",
    "\n",
    "* **instance_count (int)** – Number of Amazon EC2 instances to use for training.\n",
    "\n",
    "* **instance_type (str)** – Type of EC2 instance to use for training, for example, ‘ml.c4.xlarge’.\n",
    "\n",
    "* **volume_size (int)** – Size in GB of the EBS volume to use for storing input data during training (default: 30). Must be large enough to store training data if File Mode is used (which is the default).\n",
    "\n",
    "* **model_uri (str)** – URI where a pre-trained model is stored, either locally or in S3 (default: None). If specified, the estimator will create a channel pointing to the model so the training job can download it. This model can be a ‘model.tar.gz’ from a previous training job, or other artifacts coming from a different source. In local mode, this should point to the path in which the model is located and not the file itself, as local Docker containers will try to mount the URI as a volume.\n",
    "\n",
    "* **output_path (str)** - S3 location for saving the training result (model artifacts and output files). If not specified, results are stored to a default bucket. If the bucket with the specific name does not exist, the estimator creates the bucket during the fit() method execution. file:// urls are used for local mode. For example: ‘file://model/’ will save to the model folder in the current directory."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define the hyperparamters for EC2 training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "hyperparameters = {\r\n",
    "    \"seed\": 42,\r\n",
    "    #   \"fast\": false,\r\n",
    "    #   \"mixup\": false,\r\n",
    "    #   \"balance\": false,\r\n",
    "    #   \"balance_datasets\": false,\r\n",
    "    #   \"swa\": false,\r\n",
    "    #   \"show\": false,\r\n",
    "    #   \"use_idrid\": false,\r\n",
    "    #   \"use_messidor\": false,\r\n",
    "    #   \"use_aptos2015\": false,\r\n",
    "    # \"use_aptos2019\": \"\",\r\n",
    "    # \"verbose\": \"\",\r\n",
    "    #   \"coarse\": false,\r\n",
    "    \"accumulation-steps\": 1,\r\n",
    "    \"data-dir\": \"/opt/ml/input/data\",\r\n",
    "    \"model\": \"seresnext50d_gwap\",\r\n",
    "    \"batch-size\": 4,\r\n",
    "    \"epochs\": 100,\r\n",
    "    \"early-stopping\": 10,\r\n",
    "    # \"fold\": [\r\n",
    "    #     0,\r\n",
    "    #     1,\r\n",
    "    #     2,\r\n",
    "    #     3\r\n",
    "    # ],\r\n",
    "    #   \"freeze_encoder\": false,\r\n",
    "    \"learning-rate\": 0.0001,\r\n",
    "    # \"criterion_reg\": [\r\n",
    "    #     \"mse\"\r\n",
    "    # ],\r\n",
    "    #   \"criterion_ord\": null,\r\n",
    "    # \"criterion_cls\": [\r\n",
    "    #     \"focal_kappa\"\r\n",
    "    # ],\r\n",
    "    \"l1\": 0.0002,\r\n",
    "    # \"l2\": 0,\r\n",
    "    \"optimizer\": \"AdamW\",\r\n",
    "    #   \"preprocessing\": null,\r\n",
    "    #   \"checkpoint\": null,\r\n",
    "      \"workers\": 8,\r\n",
    "    \"augmentations\": \"medium\",\r\n",
    "    #   \"tta\": null,\r\n",
    "#     \"transfer\": \"pretrained\",\r\n",
    "    #   \"fp16\": true,\r\n",
    "    \"scheduler\": \"multistep\",\r\n",
    "    \"size\": 1024,\r\n",
    "    \"weight-decay\": 0.0001,\r\n",
    "    #   \"weight_decay_step\": null,\r\n",
    "#     \"dropout\": 0.2,\r\n",
    "    # \"warmup\": 0,\r\n",
    "    #   \"experiment\": null\r\n",
    "}\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define the estimator configuration for EC2 training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "estimator_config = {\r\n",
    "    'entry_point': 'train_reg_1.py',\r\n",
    "    'source_dir': source_proj_dir,\r\n",
    "    'framework_version': '1.6.0',\r\n",
    "    'py_version': 'py3',\r\n",
    "    'instance_count': 1,\r\n",
    "    'instance_type': 'ml.p3.16xlarge',\r\n",
    "    'volume_size':200,\r\n",
    "    'role': sagemaker.get_execution_role(),\r\n",
    "    'output_path': f's3://{bucket}/training_job',\r\n",
    "    'checkpoint_local_path': None,\r\n",
    "    'max_run':86400*3,\r\n",
    "    'hyperparameters': hyperparameters,\r\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create the estimator configured for EC2 training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "pytorch_estimator = PyTorch(**estimator_config)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define the data channels using the proper S3 URIs"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "data_channels = {\r\n",
    "    'aptos-2015': f's3://{bucket}/aptos-2015',\r\n",
    "    'aptos-2019':   f's3://{bucket}/aptos-2019',\r\n",
    "    'idrid':   f's3://{bucket}/idrid',\r\n",
    "    'messidor':   f's3://{bucket}/messidor',\r\n",
    "    'origa':   f's3://{bucket}/origa',\r\n",
    "    'stare':   f's3://{bucket}/stare',\r\n",
    "#     'pretrained':   f's3://{bucket}/pretrained', # already loading the pretrained after downloading \r\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "# pytorch_estimator.fit(data_channels)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# SageMaker endpoint\n",
    "\n",
    "To deploy the model you previously trained, you need to create a Sagemaker Endpoint. This is a hosted prediction service that you can use to perform inference.\n",
    "\n",
    "## Finding the model\n",
    "\n",
    "This notebook uses a stored model if it exists. If you recently ran a training example that use the `%store%` magic, it will be restored in the next cell.\n",
    "\n",
    "Otherwise, you can pass the URI to the model file (a .tar.gz file) in the `model_data` variable.\n",
    "\n",
    "You can find your model files through the [SageMaker console](https://console.aws.amazon.com/sagemaker/home) by choosing **Training > Training jobs** in the left navigation pane. Find your recent training job, choose it, and then look for the `s3://` link in the **Output** pane. Uncomment the model_data line in the next cell that manually sets the model's URI."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "# Retrieve a saved model from a previous notebook run's stored variable\r\n",
    "# %store -r model_data\r\n",
    "os.chdir(\"/home/ec2-user/SageMaker/Retinopathy2\")\r\n",
    "\r\n",
    "# If no model was found, set it manually here.\r\n",
    "model_data = 's3://dataset-retinopathy/pytorch-training-2020-12-29-09-38-13-247/source/sourcedir.tar.gz'\r\n",
    "\r\n",
    "print(\"Using this model: {}\".format(model_data))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using this model: s3://dataset-retinopathy/pytorch-training-2020-12-29-09-38-13-247/source/sourcedir.tar.gz\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create a model object\n",
    "\n",
    "You define the model object by using SageMaker SDK's `PyTorchModel` and pass in the model from the `estimator` and the `entry_point`. The endpoint's entry point for inference is defined by `model_fn` as seen in the following code block that prints out `inference.py`. The function loads the model and sets it to use a GPU, if available."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load the weights back into a PyTorch model\n",
    "Since the model was trained on a GPU we need to use the `map_location=torch.device('cpu')` kwarg to load the model on a CPU backed notebook instance."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "import sagemaker\r\n",
    "role = sagemaker.get_execution_role()\r\n",
    "\r\n",
    "from sagemaker.pytorch import PyTorchModel\r\n",
    "model = PyTorchModel(model_data=model_data, source_dir='code',\r\n",
    "                        entry_point='inference.py', role=role, framework_version='1.6.0', py_version='py3')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Deploy the model on an endpoint\n",
    "\n",
    "You create a `predictor` by using the `model.deploy` function. You can optionally change both the instance count and instance type."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "predictor = model.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'code'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-9a751cfbcc32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeploy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_instance_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ml.m4.xlarge'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/sagemaker/model.py\u001b[0m in \u001b[0;36mdeploy\u001b[0;34m(self, initial_instance_count, instance_type, serializer, deserializer, accelerator_type, endpoint_name, tags, kms_key, wait, data_capture_config, **kwargs)\u001b[0m\n\u001b[1;32m    750\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_base_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"-\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_base_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompiled_model_suffix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 752\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_sagemaker_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccelerator_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    753\u001b[0m         production_variant = sagemaker.production_variant(\n\u001b[1;32m    754\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_instance_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccelerator_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccelerator_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/sagemaker/model.py\u001b[0m in \u001b[0;36m_create_sagemaker_model\u001b[0;34m(self, instance_type, accelerator_type, tags)\u001b[0m\n\u001b[1;32m    304\u001b[0m                 \u001b[0;34m/\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlatest\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mreference\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mservices\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0msagemaker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;31m#SageMaker.Client.add_tags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m         \"\"\"\n\u001b[0;32m--> 306\u001b[0;31m         \u001b[0mcontainer_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_container_def\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccelerator_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccelerator_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_base_name_if_needed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontainer_def\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Image\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/sagemaker/pytorch/model.py\u001b[0m in \u001b[0;36mprepare_container_def\u001b[0;34m(self, instance_type, accelerator_type)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0mdeploy_key_prefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_code_key_prefix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeploy_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_upload_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeploy_key_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_mms_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m         \u001b[0mdeploy_env\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0mdeploy_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_framework_env_vars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/sagemaker/model.py\u001b[0m in \u001b[0;36m_upload_code\u001b[0;34m(self, key_prefix, repack)\u001b[0m\n\u001b[1;32m   1129\u001b[0m                 \u001b[0mrepacked_model_uri\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrepacked_model_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m                 \u001b[0msagemaker_session\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m                 \u001b[0mkms_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_kms_key\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1132\u001b[0m             )\n\u001b[1;32m   1133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/sagemaker/utils.py\u001b[0m in \u001b[0;36mrepack_model\u001b[0;34m(inference_script, source_directory, dependencies, model_uri, repacked_model_uri, sagemaker_session, kms_key)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m         _create_or_update_code_dir(\n\u001b[0;32m--> 426\u001b[0;31m             \u001b[0mmodel_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minference_script\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_directory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdependencies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msagemaker_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m         )\n\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/sagemaker/utils.py\u001b[0m in \u001b[0;36m_create_or_update_code_dir\u001b[0;34m(model_dir, inference_script, source_directory, dependencies, sagemaker_session, tmp)\u001b[0m\n\u001b[1;32m    480\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m             \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m         \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopytree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_directory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/shutil.py\u001b[0m in \u001b[0;36mcopytree\u001b[0;34m(src, dst, symlinks, ignore, copy_function, ignore_dangling_symlinks)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m     \"\"\"\n\u001b[0;32m--> 315\u001b[0;31m     \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mignore\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0mignored_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mignore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'code'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test the model\n",
    "You can test the depolyed model using samples from the test set.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Download the test set\r\n",
    "from torchvision import datasets, transforms\r\n",
    "from torch.utils.data import DataLoader\r\n",
    "\r\n",
    "test_set = datasets.MNIST('data', download=True, train=False, \r\n",
    "                          transform=transforms.Compose([\r\n",
    "                            transforms.ToTensor(),\r\n",
    "                            transforms.Normalize((0.1307,), (0.3081,))\r\n",
    "                            ]))\r\n",
    "\r\n",
    "\r\n",
    "# Randomly sample 16 images from the test set\r\n",
    "test_loader = DataLoader(test_set, shuffle=True, batch_size=16)\r\n",
    "test_images, _ = iter(test_loader).next()\r\n",
    "\r\n",
    "# inspect the images\r\n",
    "import torchvision\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "%matplotlib inline\r\n",
    "\r\n",
    "def imshow(img):\r\n",
    "    img = img.numpy()\r\n",
    "    img = np.transpose(img, (1, 2, 0))\r\n",
    "    plt.imshow(img)\r\n",
    "    return\r\n",
    "\r\n",
    "# unnormalize the test images for displaying\r\n",
    "unnorm_images = (test_images * 0.3081) + 0.1307\r\n",
    "\r\n",
    "print(\"Sampled test images: \")\r\n",
    "imshow(torchvision.utils.make_grid(unnorm_images))\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Send the sampled images to endpoint for inference\r\n",
    "outputs = predictor.predict(test_images.numpy())\r\n",
    "predicted = np.argmax(outputs, axis=1)\r\n",
    "\r\n",
    "print(\"Predictions: \")\r\n",
    "print(predicted.tolist())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cleanup\n",
    "\n",
    "If you don't intend on trying out inference or to do anything else with the endpoint, you should delete it."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "predictor.delete_endpoint()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}