{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Kaggle data management api"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\r\n",
    "import os, sys\r\n",
    "\r\n",
    "# !pip install kaggle # now download the api token and store it to /home/ec2-user/.kaggle/kaggle.json\r\n",
    "# !cd /home/ec2-user/SageMaker\r\n",
    "# !mkdir /home/ec2-user/.kaggle/\r\n",
    "# !mv /home/ec2-user/SageMaker/kaggle.json /home/ec2-user/.kaggle/kaggle.json\r\n",
    "# !chmod 600 /home/ec2-user/.kaggle/kaggle.json # for privacy\r\n",
    "!df -h --total  # get the harddisk info\r\n",
    "!nvcc --version  # Get the cuda version\r\n",
    "os.chdir(\"/home/ec2-user/SageMaker\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# for root, dirs, files in os.walk(\"CervicalCancer_dataset\", topdown = False):\r\n",
    "#     print(len(files))\r\n",
    "# #    for name in files:\r\n",
    "# #       print(os.path.join(root, name))\r\n",
    "#     for name in dirs:\r\n",
    "#         print(os.path.join(root, name))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "### running for setting up the git repository\r\n",
    "!git clone https://github.com/RamsteinWR/IntelCervicalCancer.git\r\n",
    "# !rm -rf /home/ec2-user/SageMaker/IntelCervicalCancer/data\r\n",
    "# !mv /home/ec2-user/SageMaker/IntelCervicalCancerData /home/ec2-user/SageMaker/IntelCervicalCancer/data\r\n",
    "\r\n",
    "# import shutil\r\n",
    "# shutil.rmtree(\"/home/ec2-user/SageMaker/Haemorrhage_dataset\", ignore_errors=True)\r\n",
    "##### run this part for removing the repository\r\n",
    "# !mv /home/ec2-user/SageMaker/IntelCervicalCancer/data /home/ec2-user/SageMaker/IntelCervicalCancerData\r\n",
    "# !rm -rf /home/ec2-user/SageMaker/IntelCervicalCancer\r\n",
    "\r\n",
    "# !pip install py7zr\r\n",
    "# import py7zr\r\n",
    "# with py7zr.SevenZipFile('CervicalCancer_dataset.7z', 'w') as archive:\r\n",
    "#     archive.writeall('/home/ec2-user/SageMaker/CervicalCancer_dataset', 'base')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cloning into 'IntelCervicalCancer'...\n",
      "remote: Enumerating objects: 305, done.\u001b[K\n",
      "remote: Counting objects: 100% (305/305), done.\u001b[K\n",
      "remote: Compressing objects: 100% (180/180), done.\u001b[K\n",
      "remote: Total 305 (delta 199), reused 224 (delta 121), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (305/305), 1.53 MiB | 28.01 MiB/s, done.\n",
      "Resolving deltas: 100% (199/199), done.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "os.chdir(\"/home/ec2-user/SageMaker\")\r\n",
    "\r\n",
    "# Download the challenge data here \r\n",
    "# !kaggle competitions download -c intel-mobileodt-cervical-cancer-screening\r\n",
    "# !unzip -o -q /home/ec2-user/SageMaker/intel-mobileodt-cervical-cancer-screening.zip -d /home/ec2-user/SageMaker/IntelCervicalCancer/data  #-q for quitely no verbose\r\n",
    "# !rm -rf /home/ec2-user/SageMaker/intel-mobileodt-cervical-cancer-screening.zip"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# !mv /home/ec2-user/SageMaker/PneumoniaData/stage_2_detailed_class_info.csv /home/ec2-user/SageMaker/PneumoniaData/detailed_class_info.csv\r\n",
    "# !mv /home/ec2-user/SageMaker/PneumoniaData/stage_2_test_images /home/ec2-user/SageMaker/PneumoniaData/test_dicoms\r\n",
    "\r\n",
    "# !mv /home/ec2-user/SageMaker/IntelCervicalCancer/data/test_stg2/* /home/ec2-user/SageMaker/IntelCervicalCancer/data/test/\r\n",
    "# !rm -rf IntelCervicalCancer/data/train1\r\n",
    "\r\n",
    "# !pip install py7zr\r\n",
    "# import py7zr\r\n",
    "# with py7zr.SevenZipFile('/home/ec2-user/SageMaker/IntelCervicalCancer/data/test_stg2.7z', mode='r', password='byecervicalcancer') as z:\r\n",
    "#     z.extractall()\r\n",
    "\r\n",
    "# !zip -r -0 -q CervicalCancer_dataset.zip /home/ec2-user/SageMaker/CervicalCancer_dataset\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessing the authenication management"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "%%time\r\n",
    "import boto3\r\n",
    "import botocore\r\n",
    "from botocore.exceptions import ClientError\r\n",
    "from tqdm import tqdm\r\n",
    "import os \r\n",
    "import urllib.request\r\n",
    "import re\r\n",
    "import sagemaker\r\n",
    "from sagemaker import get_execution_role\r\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\r\n",
    "\r\n",
    "role = get_execution_role()\r\n",
    "\r\n",
    "bucket = \"cervical-cancer-dataset\"\r\n",
    "region_name=\"us-east-1\"\r\n",
    "aws_access_key_id = \"ahiguhgiuf\"\r\n",
    "aws_secret_access_key = \"hagudfyguyagufygu\"\r\n",
    "\r\n",
    "training_image = get_image_uri(boto3.Session().region_name, 'image-classification')\r\n",
    "\r\n",
    "def download(url):\r\n",
    "    filename = url.split(\"/\")[-1]\r\n",
    "    if not os.path.exists(filename):\r\n",
    "        urllib.request.urlretrieve(url, filename)\r\n",
    "\r\n",
    "def upload_to_s3(channel, file):\r\n",
    "    s3 = boto3.resource('s3',\r\n",
    "                             region_name=region_name,\r\n",
    "                             aws_access_key_id=aws_access_key_id,\r\n",
    "                             aws_secret_access_key=aws_secret_access_key)    \r\n",
    "    data = open(file, \"rb\")\r\n",
    "    key = channel + '/' + file\r\n",
    "    print(\"Uploading file {} to s3://{}/{}\".format(file, bucket, channel))\r\n",
    "    s3.Bucket(bucket).put_object(Key=key, Body=data)\r\n",
    "\r\n",
    "def upload_dir_to_s3(bucket, s3_folder, dir_to_upload):\r\n",
    "    s3_client = boto3.client('s3',\r\n",
    "                             region_name=region_name,\r\n",
    "                             aws_access_key_id=aws_access_key_id,\r\n",
    "                             aws_secret_access_key=aws_secret_access_key)\r\n",
    "    print(\"Uploading {} to s3://{}/{}\".format(dir_to_upload, bucket, s3_folder))\r\n",
    "    # enumerate local files recursively\r\n",
    "    for root, dirs, files in os.walk(dir_to_upload):\r\n",
    "        for filename in tqdm(files):\r\n",
    "            # construct the full local path\r\n",
    "            local_path = os.path.join(root, filename)\r\n",
    "            # construct the full Dropbox path\r\n",
    "            relative_path = os.path.relpath(local_path, dir_to_upload)\r\n",
    "            s3_path = os.path.join(s3_folder, relative_path).replace(\"\\\\\", \"/\")\r\n",
    "            try:\r\n",
    "                s3_client.head_object(Bucket=bucket, Key=s3_path)\r\n",
    "                print(\"Path found on S3! Deleting %s...\" % s3_path)\r\n",
    "                try:\r\n",
    "                    s3_client.delete_object(Bucket=bucket, Key=s3_path)\r\n",
    "                    try:\r\n",
    "#                         print(\"Uploading {} to s3://{}/{}\".format(dir_to_upload, bucket, s3_path)\r\n",
    "                        s3_client.upload_file(local_path, Bucket=bucket, Key=s3_path)\r\n",
    "                    except ClientError as e:\r\n",
    "                        logging.error(e)\r\n",
    "                except:\r\n",
    "                    print(\"Unable to delete from s3 %s...\" % s3_path)\r\n",
    "            except:\r\n",
    "                try:\r\n",
    "                    s3_client.upload_file(local_path, Bucket=bucket, Key=s3_path)\r\n",
    "                except ClientError as e:\r\n",
    "                    logging.error(e)\r\n",
    "    print(\"Upload completed successfully.\")\r\n",
    "    \r\n",
    "def download_dir(s3_folder, local_path, bucket=\"\"):\r\n",
    "    \"\"\"\r\n",
    "    params:\r\n",
    "    - s3_folder: pattern to match in s3\r\n",
    "    - local_path: local_path path to folder in which to place files\r\n",
    "    - bucket: s3 bucket with target contents\r\n",
    "    - client: initialized s3 client object\r\n",
    "    \"\"\"\r\n",
    "    client = boto3.client('s3', region_name=region_name)\r\n",
    "    keys = []\r\n",
    "    dirs = []\r\n",
    "    next_token = ''\r\n",
    "    base_kwargs = {\r\n",
    "        'Bucket': bucket,\r\n",
    "        'Prefix': s3_folder,\r\n",
    "    }\r\n",
    "    while next_token is not None:\r\n",
    "        kwargs = base_kwargs.copy()\r\n",
    "        if next_token != '':\r\n",
    "            kwargs.update({'ContinuationToken': next_token})\r\n",
    "        results = client.list_objects_v2(**kwargs)\r\n",
    "        contents = results.get('Contents')\r\n",
    "        for i in contents:\r\n",
    "            k = i.get('Key')\r\n",
    "            if k[-1] != '/':\r\n",
    "                keys.append(k)\r\n",
    "            else:\r\n",
    "                dirs.append(k)\r\n",
    "        next_token = results.get('NextContinuationToken')\r\n",
    "    for d in dirs:\r\n",
    "        dest_pathname = os.path.join(local_path, d)\r\n",
    "        if not os.path.exists(os.path.dirname(dest_pathname)):\r\n",
    "            os.makedirs(os.path.dirname(dest_pathname))\r\n",
    "    print(\"{} files found in {} directories. Downloading now...\".format(len(keys), len(dirs)))\r\n",
    "    for k in tqdm(keys):\r\n",
    "        dest_pathname = os.path.join(local_path, k)\r\n",
    "        if not os.path.exists(os.path.dirname(dest_pathname)):\r\n",
    "            os.makedirs(os.path.dirname(dest_pathname))\r\n",
    "        try:\r\n",
    "#             print(\"Downloading {}\".format(dest_pathname))\r\n",
    "            client.download_file(bucket, k, dest_pathname)\r\n",
    "        except botocore.exceptions.ClientError as e:\r\n",
    "            if e.response['Error']['Code'] == \"404\":\r\n",
    "                print(\"The object does not exist.\")\r\n",
    "            else:\r\n",
    "                raise\r\n",
    "    print(\"{} files downloaded successfully.\".format(len(keys)))\r\n",
    "    \r\n",
    "# download('http://data.lip6.fr/cadene/pretrained/home/ec2-user/SageMaker/rsna19/models/se_resnext50_32x4d-a260b3a4.pth')\r\n",
    "# upload_to_s3(\"pretrained\", 'se_resnext50_32x4d-a260b3a4.pth')\r\n",
    "# upload_dir_to_s3(bucket=bucket, s3_folder='aptos-2019', dir_to_upload=\"/home/ec2-user/SageMaker/data\")\r\n",
    "\r\n",
    "# download_dir(s3_folder='aptos-2015/train_images_768/', local_path='/home/ec2-user/SageMaker/data/aptos-2015/train_images_768/', bucket=bucket)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "The method get_image_uri has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "Defaulting to the only supported framework/algorithm version: 1. Ignoring framework/algorithm version: 1.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 2.8 s, sys: 3.44 s, total: 6.24 s\n",
      "Wall time: 636 ms\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### multipart uploader"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "%%time\r\n",
    "import argparse\r\n",
    "import os\r\n",
    "\r\n",
    "import boto3\r\n",
    "\r\n",
    "\r\n",
    "class S3MultipartUpload(object):\r\n",
    "    # AWS throws EntityTooSmall error for parts smaller than 5 MB\r\n",
    "    PART_MINIMUM = int(5e6)\r\n",
    "\r\n",
    "    def __init__(self,\r\n",
    "                 bucket,\r\n",
    "                 key,\r\n",
    "                 local_path,\r\n",
    "                 aws_access_key_id,\r\n",
    "                 aws_secret_access_key,\r\n",
    "                 part_size=int(6e6),\r\n",
    "                 profile_name=None,\r\n",
    "                 region_name=\"us-east-1\",\r\n",
    "                 verbose=False): # keep it false unless it will verbose all connnectivity logs\r\n",
    "        self.bucket = bucket\r\n",
    "        self.key = key\r\n",
    "        self.path = local_path\r\n",
    "        self.total_bytes = os.stat(local_path).st_size\r\n",
    "        self.part_bytes = part_size\r\n",
    "#         assert part_size > self.PART_MINIMUM\r\n",
    "#         assert (self.total_bytes % part_size == 0\r\n",
    "#                 or self.total_bytes % part_size > self.PART_MINIMUM)\r\n",
    "        self.s3 = boto3.session.Session(aws_access_key_id=aws_access_key_id,\r\n",
    "                                        aws_secret_access_key=aws_secret_access_key,\r\n",
    "                                        region_name=region_name).client(\"s3\")\r\n",
    "        if verbose:\r\n",
    "            boto3.set_stream_logger(name=\"botocore\")\r\n",
    "\r\n",
    "    def abort_all(self):\r\n",
    "        mpus = self.s3.list_multipart_uploads(Bucket=self.bucket)\r\n",
    "        aborted = []\r\n",
    "        print(\"Aborting\", len(mpus), \"uploads\")\r\n",
    "        if \"Uploads\" in mpus:\r\n",
    "            for u in mpus[\"Uploads\"]:\r\n",
    "                upload_id = u[\"UploadId\"]\r\n",
    "                aborted.append(\r\n",
    "                    self.s3.abort_multipart_upload(\r\n",
    "                        Bucket=self.bucket, Key=self.key, UploadId=upload_id))\r\n",
    "        return aborted\r\n",
    "\r\n",
    "    def create(self):\r\n",
    "        mpu = self.s3.create_multipart_upload(Bucket=self.bucket, Key=self.key)\r\n",
    "        mpu_id = mpu[\"UploadId\"]\r\n",
    "        return mpu_id\r\n",
    "\r\n",
    "    def upload(self, mpu_id):\r\n",
    "        parts = []\r\n",
    "        uploaded_bytes = 0\r\n",
    "        with open(self.path, \"rb\") as f:\r\n",
    "            i = 1\r\n",
    "            while True:\r\n",
    "                data = f.read(self.part_bytes)\r\n",
    "                if not len(data):\r\n",
    "                    break\r\n",
    "                part = self.s3.upload_part(\r\n",
    "                    Body=data, Bucket=self.bucket, Key=self.key, UploadId=mpu_id, PartNumber=i)\r\n",
    "                parts.append({\"PartNumber\": i, \"ETag\": part[\"ETag\"]})\r\n",
    "                uploaded_bytes += len(data)\r\n",
    "                print(\"{0} of {1} uploaded ({2:.3f}%)\".format(\r\n",
    "                    uploaded_bytes, self.total_bytes,\r\n",
    "                    as_percent(uploaded_bytes, self.total_bytes)))\r\n",
    "                i += 1\r\n",
    "        return parts\r\n",
    "\r\n",
    "    def complete(self, mpu_id, parts):\r\n",
    "        result = self.s3.complete_multipart_upload(\r\n",
    "            Bucket=self.bucket,\r\n",
    "            Key=self.key,\r\n",
    "            UploadId=mpu_id,\r\n",
    "            MultipartUpload={\"Parts\": parts})\r\n",
    "        return result\r\n",
    "\r\n",
    "\r\n",
    "# Helper\r\n",
    "def as_percent(num, denom):\r\n",
    "    return float(num) / float(denom) * 100.0\r\n",
    "\r\n",
    "\r\n",
    "def parse_args():\r\n",
    "    parser = argparse.ArgumentParser(description='Multipart upload')\r\n",
    "    parser.add_argument('--bucket', required=True)\r\n",
    "    parser.add_argument('--key', required=True)\r\n",
    "    parser.add_argument('--path', required=True)\r\n",
    "    parser.add_argument('--region', default=\"us-east-1\")\r\n",
    "    parser.add_argument('--profile', default=None)\r\n",
    "    return parser.parse_args()\r\n",
    "\r\n",
    "\r\n",
    "bucket = \"cervical-cancer-dataset\"\r\n",
    "region_name = \"us-east-1\"\r\n",
    "aws_access_key_id = \"haidhfaysdguyf\"\r\n",
    "aws_secret_access_key = \"huidhfoiuahdfuha\"\r\n",
    "\r\n",
    "\r\n",
    "def multipart_upload(key, local_path):\r\n",
    "    # args = parse_args()\r\n",
    "    # mpu = S3MultipartUpload(\r\n",
    "    #     args.bucket,\r\n",
    "    #     args.key,\r\n",
    "    #     args.path,\r\n",
    "    #     profile_name=args.profile,\r\n",
    "    #     region_name=args.region)\r\n",
    "    mpu = S3MultipartUpload(\r\n",
    "        aws_access_key_id=aws_access_key_id,\r\n",
    "        aws_secret_access_key=aws_secret_access_key,\r\n",
    "        bucket=bucket,\r\n",
    "        key=key,\r\n",
    "        local_path=local_path,\r\n",
    "        region_name='us-east-1')\r\n",
    "    # abort all multipart uploads for this bucket (optional, for starting over)\r\n",
    "    mpu.abort_all()\r\n",
    "    # create new multipart upload\r\n",
    "    mpu_id = mpu.create()\r\n",
    "    # upload parts\r\n",
    "    parts = mpu.upload(mpu_id)\r\n",
    "    # complete multipart upload\r\n",
    "    print(mpu.complete(mpu_id, parts))\r\n",
    "# if __name__ == \"__main__\":\r\n",
    "#     main()\r\n",
    "\r\n",
    "# multipart_upload(key='CervicalCancer_dataset/dataset_in_numpy', local_path='/home/ec2-user/SageMaker/CervicalCancer_dataset/dataset_in_numpy/X_train.npy')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 17 µs, sys: 19 µs, total: 36 µs\n",
      "Wall time: 40.5 µs\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Uploading files to s3"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id='idg4c.1'></a>\n",
    "## Dependencies\n",
    "___\n",
    "### import packages and check SageMaker version"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# !pip install -r /home/ec2-user/SageMaker/IntelCervicalCancer/requirements.txt\n",
    "# !rm -rf CervicalCancer_dataset/detect_data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import json\n",
    "import torch\n",
    "import tarfile\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision as tv\n",
    "import pathlib                          # Path management tool (standard library)\n",
    "import subprocess                       # Runs shell commands via Python (standard library)\n",
    "import sagemaker                        # SageMaker Python SDK\n",
    "from sagemaker.pytorch import PyTorch   # PyTorch Estimator for TensorFlow\n",
    "\n",
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "# S3Uploader.upload(local_path='/home/ec2-user/SageMaker/CervicalCancer_dataset/dataset_in_numpy/X_train.npy', \n",
    "#                   desired_s3_uri=\"S3://cervical-cancer-dataset/CervicalCancer_dataset/dataset_in_numpy/\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training the simple model\n",
    "### adding some modules to path"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import sys, os\n",
    "sys.path.append(\"/home/ec2-user/SageMaker/IntelCervicalCancer\")\n",
    "sys.path.append(\"/home/ec2-user/SageMaker/IntelCervicalCancer/src\")\n",
    "sys.path.append(\"/home/ec2-user/SageMaker/PneumoniaRSNA1/models/DeformableConvNets\")\n",
    "sys.path.append(\"/home/ec2-user/SageMaker/IntelCervicalCancer/src/kfold_train\")\n",
    "sys.path.append(\"/home/ec2-user/SageMaker/IntelCervicalCancer/src/scripts\")\n",
    "print(sys.path)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['', '/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python36.zip', '/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6', '/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/lib-dynload', '/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages', '/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/IPython/extensions', '/home/ec2-user/.ipython', '/home/ec2-user/SageMaker/IntelCervicalCancer', '/home/ec2-user/SageMaker/IntelCervicalCancer/src', '/home/ec2-user/SageMaker/PneumoniaRSNA1/models/DeformableConvNets', '/home/ec2-user/SageMaker/IntelCervicalCancer/src/kfold_train', '/home/ec2-user/SageMaker/IntelCervicalCancer/src/scripts']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# import os, sys\n",
    "os.chdir(\"/home/ec2-user/SageMaker/IntelCervicalCancer/\")\n",
    "\n",
    "# !python bbox_data_preproc.py        "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loaded train_val.pickle from /home/ec2-user/SageMaker/CervicalCancer_dataset/train_val.pickle\n",
      "Augmenting the data: Output being saved to /home/ec2-user/SageMaker/CervicalCancer_dataset/detect_data/data001_size512/train\n",
      "979it [08:27,  1.93it/s]\n",
      "Saved augmented data to /home/ec2-user/SageMaker/CervicalCancer_dataset/detect_data/data001_size512/train\n",
      "Augmenting the data: Output being saved to /home/ec2-user/SageMaker/CervicalCancer_dataset/detect_data/data001_size512/val\n",
      "485it [04:24,  1.83it/s]\n",
      "Saved augmented data to /home/ec2-user/SageMaker/CervicalCancer_dataset/detect_data/data001_size512/val\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "import os, sys\n",
    "os.chdir(\"/home/ec2-user/SageMaker/IntelCervicalCancer/\")\n",
    "\n",
    "# !python train_detector.py"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "14784it [01:36, 152.53it/s]\n",
      "6880it [00:45, 152.48it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"train_detector.py\", line 77, in <module>\n",
      "    N_max=15616)  # total 15664 here 15616/64=244\n",
      "  File \"/home/ec2-user/SageMaker/IntelCervicalCancer/src/scripts/bbox_data_preproc.py\", line 179, in load_dataset\n",
      "    np.save(join(dataset_in_numpy, 'X_train.npy'), X_train)\n",
      "  File \"<__array_function__ internals>\", line 6, in save\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/numpy/lib/npyio.py\", line 524, in save\n",
      "    file_ctx = open(file, \"wb\")\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/home/ec2-user/SageMaker/CervicalCancer_dataset/dataset_in_numpy/X_train.npy'\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "hyperparameters = {\n",
    "    'epochs': 10, \n",
    "    'batch-size': 64, \n",
    "    'learning-rate': 0.001,\n",
    "    'workers': 4\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "estimator_config = {\n",
    "    'entry_point': 'train_detector.py',\n",
    "    'source_dir': '/home/ec2-user/SageMaker/IntelCervicalCancer',\n",
    "    'framework_version': '1.6.0',\n",
    "    'py_version': 'py3',\n",
    "    'instance_type': 'ml.p2.8xlarge',\n",
    "    'instance_count': 1,\n",
    "    'volume_size':100,\n",
    "    'checkpoint_local_path': None,\n",
    "    'max_run':86400,\n",
    "    'role': sagemaker.get_execution_role(),\n",
    "    'output_path': 's3://cervical-cancer-dataset/training_job',\n",
    "    'hyperparameters': hyperparameters\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "pytorch_estimator = PyTorch(**estimator_config)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "data_channels = {\n",
    "    'dataset_in_numpy': 's3://cervical-cancer-dataset/CervicalCancer_dataset/dataset_in_numpy',\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "pytorch_estimator.fit(data_channels)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2021-01-04 11:45:29 Starting - Starting the training job...\n",
      "2021-01-04 11:45:53 Starting - Launching requested ML instancesProfilerReport-1609760728: InProgress\n",
      "............\n",
      "2021-01-04 11:47:54 Starting - Preparing the instances for training.........\n",
      "2021-01-04 11:49:15 Downloading - Downloading input data..................................................................\n",
      "2021-01-04 12:00:36 Training - Downloading the training image...\n",
      "2021-01-04 12:01:04 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2021-01-04 12:00:53,893 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2021-01-04 12:00:53,970 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2021-01-04 12:01:00,200 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2021-01-04 12:01:01,128 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"dataset_in_numpy\": \"/opt/ml/input/data/dataset_in_numpy\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch-size\": 64,\n",
      "        \"learning-rate\": 0.001,\n",
      "        \"epochs\": 10,\n",
      "        \"workers\": 4\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"dataset_in_numpy\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-training-2021-01-04-11-45-28-363\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://cervical-cancer-dataset/pytorch-training-2021-01-04-11-45-28-363/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train_detector\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 32,\n",
      "    \"num_gpus\": 8,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train_detector.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch-size\":64,\"epochs\":10,\"learning-rate\":0.001,\"workers\":4}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train_detector.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"dataset_in_numpy\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"dataset_in_numpy\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train_detector\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=32\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://cervical-cancer-dataset/pytorch-training-2021-01-04-11-45-28-363/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"dataset_in_numpy\":\"/opt/ml/input/data/dataset_in_numpy\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch-size\":64,\"epochs\":10,\"learning-rate\":0.001,\"workers\":4},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"dataset_in_numpy\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2021-01-04-11-45-28-363\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://cervical-cancer-dataset/pytorch-training-2021-01-04-11-45-28-363/source/sourcedir.tar.gz\",\"module_name\":\"train_detector\",\"network_interface_name\":\"eth0\",\"num_cpus\":32,\"num_gpus\":8,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train_detector.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch-size\",\"64\",\"--epochs\",\"10\",\"--learning-rate\",\"0.001\",\"--workers\",\"4\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_DATASET_IN_NUMPY=/opt/ml/input/data/dataset_in_numpy\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH-SIZE=64\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING-RATE=0.001\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=10\u001b[0m\n",
      "\u001b[34mSM_HP_WORKERS=4\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python train_detector.py --batch-size 64 --epochs 10 --learning-rate 0.001 --workers 4\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "2021-01-04 12:07:09 Uploading - Uploading generated training model\u001b[34m2021-01-04 12:07:06,188 sagemaker-training-toolkit ERROR    ExecuteUserScriptError:\u001b[0m\n",
      "\u001b[34mCommand \"/opt/conda/bin/python train_detector.py --batch-size 64 --epochs 10 --learning-rate 0.001 --workers 4\"\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"train_detector.py\", line 77, in <module>\n",
      "    N_max=15616)  # total 15664 here 15616/64=244\n",
      "  File \"/opt/ml/code/src/scripts/bbox_data_preproc.py\", line 171, in load_dataset\n",
      "    X_train = np.load(join(dataset_in_numpy, 'X_train.npy'))\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/numpy/lib/npyio.py\", line 440, in load\n",
      "    pickle_kwargs=pickle_kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/numpy/lib/format.py\", line 771, in read_array\n",
      "    array.shape = shape\u001b[0m\n",
      "\u001b[34mValueError: cannot reshape array of size 10737419232 into shape (14784,3,512,512)\u001b[0m\n",
      "\n",
      "2021-01-04 12:07:31 Failed - Training job failed\n",
      "ProfilerReport-1609760728: Stopping\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Training job pytorch-training-2021-01-04-11-45-28-363: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand \"/opt/conda/bin/python train_detector.py --batch-size 64 --epochs 10 --learning-rate 0.001 --workers 4\"\nTraceback (most recent call last):\n  File \"train_detector.py\", line 77, in <module>\n    N_max=15616)  # total 15664 here 15616/64=244\n  File \"/opt/ml/code/src/scripts/bbox_data_preproc.py\", line 171, in load_dataset\n    X_train = np.load(join(dataset_in_numpy, 'X_train.npy'))\n  File \"/opt/conda/lib/python3.6/site-packages/numpy/lib/npyio.py\", line 440, in load\n    pickle_kwargs=pickle_kwargs)\n  File \"/opt/conda/lib/python3.6/site-packages/numpy/lib/format.py\", line 771, in read_array\n    array.shape = shape\nValueError: cannot reshape array of size 10737419232 into shape (14784,3,512,512)",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-17d0754d89e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpytorch_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_channels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m    657\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 659\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_compilation_job_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   1598\u001b[0m         \u001b[0;31m# If logs are requested, call logs_for_jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1599\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"None\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1600\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1601\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1602\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mlogs_for_job\u001b[0;34m(self, job_name, wait, poll, log_type)\u001b[0m\n\u001b[1;32m   3685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3686\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3687\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_job_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TrainingJobStatus\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3688\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3689\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36m_check_job_status\u001b[0;34m(self, job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   3265\u001b[0m                 ),\n\u001b[1;32m   3266\u001b[0m                 \u001b[0mallowed_statuses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Completed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Stopped\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3267\u001b[0;31m                 \u001b[0mactual_status\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3268\u001b[0m             )\n\u001b[1;32m   3269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Training job pytorch-training-2021-01-04-11-45-28-363: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand \"/opt/conda/bin/python train_detector.py --batch-size 64 --epochs 10 --learning-rate 0.001 --workers 4\"\nTraceback (most recent call last):\n  File \"train_detector.py\", line 77, in <module>\n    N_max=15616)  # total 15664 here 15616/64=244\n  File \"/opt/ml/code/src/scripts/bbox_data_preproc.py\", line 171, in load_dataset\n    X_train = np.load(join(dataset_in_numpy, 'X_train.npy'))\n  File \"/opt/conda/lib/python3.6/site-packages/numpy/lib/npyio.py\", line 440, in load\n    pickle_kwargs=pickle_kwargs)\n  File \"/opt/conda/lib/python3.6/site-packages/numpy/lib/format.py\", line 771, in read_array\n    array.shape = shape\nValueError: cannot reshape array of size 10737419232 into shape (14784,3,512,512)"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}